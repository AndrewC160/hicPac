% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/patchwork_matrix.R
\name{patchwork_matrix}
\alias{patchwork_matrix}
\title{Patchwork matrix.}
\usage{
patchwork_matrix(
  gr_list,
  hic_file,
  pixel_tsv,
  over_write = FALSE,
  max_pixels = 6250000,
  disable_file_lock = FALSE
)
}
\arguments{
\item{gr_list}{List of GRanges (can be named) which will be retrieved, with ranges coming in order of 5' to 3' in the contig in question. Strand indicates a given segment's direction.}

\item{hic_file}{Cooler filename from which to retrieve bins and pixel data.}

\item{pixel_tsv}{TSV file in which to cache output. If <over_write> is FALSE and <pixel_tsv> exists, this file will be read from disk instead of retrieving from the Cooler file.}

\item{over_write}{Should existing <pixel_tsv> file be overwritten? Defaults to FALSE.}

\item{max_pixels}{Maximum number of pixels to retrieve for a given segment, defaults to 6.25E6. Can be increased to Inf.}

\item{disable_file_lock}{Should the locking of HDF5 files be disabled? Defaults to FALSE, but can be set to TRUE if multiple instances will be accessing the same Cooler simultaneously.}
}
\description{
Retrive bins and pixels of HiC data corresponding to a contig assembled from
a series of genomic regions. Starts with a list of genomic regions (gr_list)
from 5' to 3' whose direction is indicated by strand, and a hic Cooler file
(hic_file) from which to retrieve data.

Given the instability of some rhdf5 functions (currently), this function
requires the use of a cache system. First, a temporary directory is saved as
a file with ".tsv" replaced with "_tmp" and every region is read directly
into a TSV file in this directory. Once all TSVs have been found, these are
read using data.table::fread() and concatenated into the output table. Once
the table's existence is confirmed, the temporary directory and its contents
are recursively unlinked. Hopefully this will turn out to be excessive once
I figure out the cause of so many rhdf5 failures, but for now this will do.
Worst case scenario, files can be "ratcheted through" such that following
each crash, the process can be resumed at the last incomplete file.
}
